{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydsPE-aN1Alm"
   },
   "source": [
    "##### Copyright &copy; 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCSByGH6C7zS"
   },
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TyrY7lV0oke"
   },
   "source": [
    "# Create a TFX pipeline using templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLYriYe10okf"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This document will provide instructions to create a TensorFlow Extended (TFX) pipeline\n",
    "using *templates* which are provided with TFX Python package.\n",
    "Many of the instructions are Linux shell commands, which will run on an AI Platform Notebooks instance. Corresponding Jupyter Notebook code cells which invoke those commands using `!` are provided.\n",
    "\n",
    "You will build a pipeline using [Taxi Trips dataset](\n",
    "https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew)\n",
    "released by the City of Chicago. We strongly encourage you to try building\n",
    "your own pipeline using your dataset by utilizing this pipeline as a baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxPMeugQ0okg"
   },
   "source": [
    "## Step 1. Set up your environment.\n",
    "\n",
    "AI Platform Pipelines will prepare a development environment to build a pipeline, and a Kubeflow Pipeline cluster to run the newly built pipeline.\n",
    "\n",
    "**NOTE:** To select a particular TensorFlow version, or select a GPU instance, create a TensorFlow pre-installed instance in AI Platform Notebooks.\n",
    "\n",
    "**NOTE:** There might be some errors during package installation. For example: \n",
    "\n",
    ">\"ERROR: some-package 0.some_version.1 has requirement other-package!=2.0.,&lt;3,&gt;=1.15, but you'll have other-package 2.0.0 which is incompatible.\" Please ignore these errors at this moment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-am1yWXt0okh"
   },
   "source": [
    "Install `tfx`, `kfp`, and `skaffold`, and add installation path to the `PATH` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNiqq_kN0okj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
      "Collecting tfx==0.26.0\n",
      "  Downloading tfx-0.26.0-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-hub<0.10,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (0.9.0)\n",
      "Requirement already satisfied: docker<5,>=4.1 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (4.4.1)\n",
      "Requirement already satisfied: tensorflow-data-validation<0.27,>=0.26 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (0.26.0)\n",
      "Collecting ml-pipelines-sdk==0.26.0\n",
      "  Downloading ml_pipelines_sdk-0.26.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tfx-bsl<0.27,>=0.26.1 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (0.26.1)\n",
      "Requirement already satisfied: ml-metadata<0.27,>=0.26 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (0.26.0)\n",
      "Requirement already satisfied: keras-tuner<1.0.2,>=1 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (1.0.1)\n",
      "Requirement already satisfied: click<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (7.1.2)\n",
      "Requirement already satisfied: pyyaml<6,>=3.12 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (5.4.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (1.12.8)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.27,>=0.26 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (0.26.0)\n",
      "Requirement already satisfied: jinja2<3,>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (2.11.3)\n",
      "Collecting pyarrow<0.18,>=0.17\n",
      "  Downloading pyarrow-0.17.1-cp37-cp37m-manylinux2014_x86_64.whl (63.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 63.8 MB 58.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (3.14.0)\n",
      "Requirement already satisfied: apache-beam[gcp]!=2.26.*,<3,>=2.25 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (2.27.0)\n",
      "Requirement already satisfied: tensorflow-transform<0.27,>=0.26 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (0.26.0)\n",
      "Requirement already satisfied: attrs<21,>=19.3.0 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (20.3.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.28.1 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (1.35.0)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (2.3.0)\n",
      "Requirement already satisfied: absl-py<0.11,>=0.9 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (0.10.0)\n",
      "Requirement already satisfied: tensorflow-cloud<0.2,>=0.1 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (0.1.7)\n",
      "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2 in /opt/conda/lib/python3.7/site-packages (from tfx==0.26.0) (2.3.2)\n",
      "Collecting kubernetes<12,>=10.0.1\n",
      "  Downloading kubernetes-11.0.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 48.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mock<3.0.0,>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2.0.0)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (4.1.3)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2021.1)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.7)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.4.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (3.7.4.3)\n",
      "Collecting httplib2<0.18.0,>=0.8\n",
      "  Downloading httplib2-0.17.4-py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 4.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.14.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2.8.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.18.2)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.3.1.1)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (3.11.3)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2.5.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2.25.1)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.9.2.1)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-dlp<2,>=0.12.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.0.0)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.4.0)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (4.2.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.26.1)\n",
      "Requirement already satisfied: google-cloud-build<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.26.1)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.0.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.7.0)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.3.0)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.5.31)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.17.1)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.2.2)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.12.0)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.15.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker<5,>=4.1->tfx==0.26.0) (0.57.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx==0.26.0) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx==0.26.0) (1.22.4)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx==0.26.0) (0.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->tfx==0.26.0) (1.52.0)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->tfx==0.26.0) (53.0.0)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.16)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.2.8)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.2.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.12.3)\n",
      "Requirement already satisfied: proto-plus>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-build<3,>=2.0.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.13.0)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-build<3,>=2.0.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.3.17)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2.20)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2<3,>=2.7.3->tfx==0.26.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.0) (4.56.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.0) (0.24.1)\n",
      "Requirement already satisfied: terminaltables in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.0) (3.1.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.0) (0.8.7)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.0) (1.6.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx==0.26.0) (0.4.4)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12,>=10.0.1->tfx==0.26.0) (1.26.3)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12,>=10.0.1->tfx==0.26.0) (2020.12.5)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12,>=10.0.1->tfx==0.26.0) (1.3.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-build<3,>=2.0.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.6.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (5.5.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot<2,>=1.2.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (2.10)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (1.1.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (1.12.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (0.36.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (0.3.3)\n",
      "Collecting numpy<2,>=1.14.3\n",
      "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 30.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (3.3.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx==0.26.0) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-datasets<3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-cloud<0.2,>=0.1->tfx==0.26.0) (3.0.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (from tensorflow-cloud<0.2,>=0.1->tfx==0.26.0) (1.30.0)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation<0.27,>=0.26->tfx==0.26.0) (1.2.2)\n",
      "Requirement already satisfied: tensorflow-metadata<0.27,>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation<0.27,>=0.26->tfx==0.26.0) (0.26.0)\n",
      "Collecting joblib<0.15,>=0.12\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 44.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets<3.1.0->tensorflow-cloud<0.2,>=0.1->tfx==0.26.0) (2.3)\n",
      "Requirement already satisfied: ipython<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (7.20.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (7.6.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (3.0.16)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (2.7.4)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (4.4.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (5.0.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.18.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (3.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (5.3.4)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (5.1.2)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (6.1.11)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.8.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (4.7.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.17.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.2.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-build<3,>=2.0.0->apache-beam[gcp]!=2.26.*,<3,>=2.25->tfx==0.26.0) (0.4.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (6.2.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (22.0.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.9.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.9.2)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (1.5.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (6.0.7)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (20.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.4.*,<3,>=1.15.2->tfx==0.26.0) (3.4.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.5.2)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (3.3.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.6.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.3)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.8.4)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (1.4.3)\n",
      "Requirement already satisfied: async-generator in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (1.10)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.27,>=0.26->tfx==0.26.0) (20.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->keras-tuner<1.0.2,>=1->tfx==0.26.0) (2.1.0)\n",
      "Installing collected packages: numpy, httplib2, pyarrow, joblib, ml-pipelines-sdk, kubernetes, tfx\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tfx is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-probability 0.11.0 requires cloudpickle==1.3, but you have cloudpickle 1.6.0 which is incompatible.\n",
      "pandas-profiling 2.8.0 requires visions[type_image_path]==0.4.4, but you have visions 0.7.1 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.8 which is incompatible.\u001b[0m\n",
      "Successfully installed httplib2-0.17.4 joblib-0.14.1 kubernetes-11.0.0 ml-pipelines-sdk-0.26.0 numpy-1.18.5 pyarrow-0.17.1 tfx-0.26.0\n",
      "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
      "Collecting kfp==1.0.0\n",
      "  Downloading kfp-1.0.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (5.4.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (1.30.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.0.0) (11.0.0)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (1.26.1)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (1.6.0)\n",
      "Collecting kfp-server-api<2.0.0,>=0.2.5\n",
      "  Downloading kfp-server-api-1.3.0.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (3.2.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (0.8.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from kfp==1.0.0) (7.1.2)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.11-py2.py3-none-any.whl (9.1 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.0) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.0) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.0) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.0) (53.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==1.0.0) (4.2.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.0) (1.2.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==1.0.0) (1.3.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (1.22.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (1.52.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (3.14.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (2021.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (2.20)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.0) (20.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.0) (3.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==1.0.0) (0.17.3)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.0) (1.26.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.0) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.0) (2.8.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==1.0.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.0) (2.10)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==1.0.0) (1.12.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==1.0.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==1.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp==1.0.0) (3.1.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==1.0.0) (0.36.2)\n",
      "Building wheels for collected packages: kfp, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kfp: filename=kfp-1.0.0-py3-none-any.whl size=159769 sha256=0eeea701b0c44d4fb7c3f00e67de8c1a6f7583235e25b0b836623bff634e34e7\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/81/39/f2/ee01d785a5bd135e42e7721fedb05857badf763fc465a4e822\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.3.0-py3-none-any.whl size=108020 sha256=0cf881ee2da7477f8c1548472636a9b6118e9e24e9c167e3c89203f2fd7898a0\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/40/c1/57/7c3f9134d56eda563ad945a904e9e2edbd22479b292558659e\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=2f1d262a32caaed251a7472dfa527a24a5f2b7dfb6cbf0b315c04e02a57b8420\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Successfully built kfp kfp-server-api strip-hints\n",
      "Installing collected packages: strip-hints, requests-toolbelt, kfp-server-api, Deprecated, kfp\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts dsl-compile and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.11 kfp-1.0.0 kfp-server-api-1.3.0 requests-toolbelt-0.9.1 strip-hints-0.1.9\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 50.1M  100 50.1M    0     0  71.2M      0 --:--:-- --:--:-- --:--:-- 71.1M\n"
     ]
    }
   ],
   "source": [
    "# Install tfx and kfp Python packages.\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --user --upgrade -q -v --log /tmp/pip.log  --use-feature=2020-resolver tfx==0.26.0\n",
    "!{sys.executable} -m pip install --user --upgrade -q -v --log /tmp/pip.log  --use-feature=2020-resolver kfp==1.0.0\n",
    "# Download skaffold and set it executable.\n",
    "!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && chmod +x skaffold && mv skaffold /home/jupyter/.local/bin/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43ncix2Q0okm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include user python binary directory and a directory containing `skaffold`.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hX1rqpbQ0okp"
   },
   "source": [
    "Let's check the versions of TFX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAIoKMNG0okq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.26.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"from tfx import version ; print('TFX version: {}'.format(version.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7JLpaXT0okv"
   },
   "source": [
    "In AI Platform Pipelines, TFX is running in a hosted Kubernetes environment using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/).\n",
    "\n",
    "Let's set some environment variables to use Kubeflow Pipelines.\n",
    "\n",
    "First, get your GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw3nsooU0okv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_CLOUD_PROJECT=cloud-training-281409\n",
      "GCP project ID:cloud-training-281409\n"
     ]
    }
   ],
   "source": [
    "# Read GCP project id from env.\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GOOGLE_CLOUD_PROJECT=shell_output[0]\n",
    "%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}\n",
    "print(\"GCP project ID:\" + GOOGLE_CLOUD_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_6r4uzE0oky"
   },
   "source": [
    "We also need to access your KFP cluster. You can access it in your Google Cloud Console under \"AI Platform > Pipeline\" menu. The \"endpoint\" of the KFP cluster can be found from the URL of the Pipelines dashboard, or you can get it from the URL of the Getting Started page where you launched this notebook. Let's create an `ENDPOINT` environment variable and set it to the KFP cluster endpoint. **ENDPOINT should contain only the hostname part of the URL.** For example, if the URL of the KFP dashboard is `https://1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com/#/start`, ENDPOINT value becomes `1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com`.\n",
    "\n",
    ">**NOTE: You MUST set your ENDPOINT value below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzqEQORV0oky"
   },
   "outputs": [],
   "source": [
    "# This refers to the KFP cluster endpoint\n",
    "ENDPOINT='https://3b47c1a069ee9385-dot-us-west1.pipelines.googleusercontent.com' # Enter your ENDPOINT here.\n",
    "if not ENDPOINT:\n",
    "    from absl import logging\n",
    "    logging.error('Set your ENDPOINT in this cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6T-KXeA0ok3"
   },
   "source": [
    "Set the image name as `titanic_pipeline` under the current GCP project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic_pipeline3\n",
      "titanic_pipeline\n"
     ]
    }
   ],
   "source": [
    "import pipeline.configs as configs\n",
    "#PIPELINE_NAME=configs.PIPELINE_NAME\n",
    "#print(PIPELINE_NAME)\n",
    "PIPELINE_NAME=\"titanic_pipeline\"\n",
    "print(PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker image name for the pipeline image.\n",
    "CUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/' + PIPELINE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!rm kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOsQbkky0ok7"
   },
   "source": [
    "And, it's done. We are ready to create a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cxlbi1QM0ok8"
   },
   "source": [
    "## Step 2. Copy the predefined template to your project directory.\n",
    "\n",
    "In this step, we will create a working pipeline project directory and files by copying additional files from a predefined template.\n",
    "\n",
    "You may give your pipeline a different name by changing the `PIPELINE_NAME` below. This will also become the name of the project directory where your files will be put.\n",
    "\n",
    "We don't do that here, since template was already copied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozHIomcd0olB"
   },
   "source": [
    "TFX includes the `taxi` template with the TFX python package. If you are planning to solve a point-wise prediction problem, including classification and regresssion, this template could be used as a starting point.\n",
    "\n",
    "The `tfx template copy` CLI command copies predefined template files into your project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxOT19QS0olH"
   },
   "source": [
    "Change the working directory context in this notebook to the project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tEYUQxH0olO"
   },
   "source": [
    ">NOTE: Don't forget to change directory in `File Browser` on the left by clicking into the project directory once it is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzT2PFrN0olQ"
   },
   "source": [
    "## Step 3. Browse your copied source files\n",
    "\n",
    "The TFX template provides basic scaffold files to build a pipeline, including Python source code, sample data, and Jupyter Notebooks to analyse the output of the pipeline. The `taxi` template uses the same *Chicago Taxi* dataset and ML model as the [Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop).\n",
    "\n",
    "Here is brief introduction to each of the Python files.\n",
    "-   `pipeline` - This directory contains the definition of the pipeline\n",
    "    -   `configs.py` — defines common constants for pipeline runners\n",
    "    -   `pipeline.py` — defines TFX components and a pipeline\n",
    "-   `models` - This directory contains ML model definitions.\n",
    "    -   `features.py`, `features_test.py` — defines features for the model\n",
    "    -   `preprocessing.py`, `preprocessing_test.py` — defines preprocessing\n",
    "        jobs using `tf::Transform`\n",
    "    -   `estimator` - This directory contains an Estimator based model.\n",
    "        -   `constants.py` — defines constants of the model\n",
    "        -   `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
    "    -   `keras` - This directory contains a Keras based model.\n",
    "        -   `constants.py` — defines constants of the model\n",
    "        -   `model.py`, `model_test.py` — defines DNN model using Keras\n",
    "-   `local_runner.py`, `kubeflow_runner.py` — define runners for each orchestration engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ROwHAsDK0olT"
   },
   "source": [
    "You might notice that there are some files with `_test.py` in their name. These are unit tests of the pipeline and it is recommended to add more unit tests as you implement your own pipelines.\n",
    "You can run unit tests by supplying the module name of test files with `-m` flag. You can usually get a module name by deleting `.py` extension and replacing `/` with `.`.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0cMdE2Z0olU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.7.9: /opt/conda/bin/python\n",
      "[ RUN      ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testNumberOfBucketFeatureBucketCount): 0.0s\n",
      "I0220 22:15:00.970785 140096448493376 test_util.py:1973] time(__main__.FeaturesTest.testNumberOfBucketFeatureBucketCount): 0.0s\n",
      "[       OK ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "[ RUN      ] FeaturesTest.testTransformedNames\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testTransformedNames): 0.0s\n",
      "I0220 22:15:00.971262 140096448493376 test_util.py:1973] time(__main__.FeaturesTest.testTransformedNames): 0.0s\n",
      "[       OK ] FeaturesTest.testTransformedNames\n",
      "[ RUN      ] FeaturesTest.test_session\n",
      "[  SKIPPED ] FeaturesTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.001s\n",
      "\n",
      "OK (skipped=1)\n",
      "Running tests under Python 3.7.9: /opt/conda/bin/python\n",
      "[ RUN      ] ModelTest.testBuildKerasModel\n",
      "2021-02-20 22:15:09.351002: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
      "2021-02-20 22:15:09.351498: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5628dc996ae0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-02-20 22:15:09.351542: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-02-20 22:15:09.354529: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "I0220 22:15:09.559078 140492120463168 layer_utils.py:192] Model: \"functional_1\"\n",
      "I0220 22:15:09.559301 140492120463168 layer_utils.py:193] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.559420 140492120463168 layer_utils.py:190] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "I0220 22:15:09.559518 140492120463168 layer_utils.py:195] ==================================================================================================\n",
      "I0220 22:15:09.559763 140492120463168 layer_utils.py:190] Age_xf (InputLayer)             [(None,)]            0                                            \n",
      "I0220 22:15:09.559868 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.560064 140492120463168 layer_utils.py:190] Embarked_xf (InputLayer)        [(None,)]            0                                            \n",
      "I0220 22:15:09.560256 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.560616 140492120463168 layer_utils.py:190] Fare_xf (InputLayer)            [(None,)]            0                                            \n",
      "I0220 22:15:09.560728 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.560919 140492120463168 layer_utils.py:190] Parch_xf (InputLayer)           [(None,)]            0                                            \n",
      "I0220 22:15:09.561018 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.561216 140492120463168 layer_utils.py:190] Pclass_xf (InputLayer)          [(None,)]            0                                            \n",
      "I0220 22:15:09.561301 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.561517 140492120463168 layer_utils.py:190] Sex_xf (InputLayer)             [(None,)]            0                                            \n",
      "I0220 22:15:09.561603 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.561785 140492120463168 layer_utils.py:190] SibSp_xf (InputLayer)           [(None,)]            0                                            \n",
      "I0220 22:15:09.561861 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.562167 140492120463168 layer_utils.py:190] dense_features (DenseFeatures)  (None, 2)            0           Age_xf[0][0]                     \n",
      "I0220 22:15:09.562285 140492120463168 layer_utils.py:190]                                                                  Embarked_xf[0][0]                \n",
      "I0220 22:15:09.562374 140492120463168 layer_utils.py:190]                                                                  Fare_xf[0][0]                    \n",
      "I0220 22:15:09.562443 140492120463168 layer_utils.py:190]                                                                  Parch_xf[0][0]                   \n",
      "I0220 22:15:09.562522 140492120463168 layer_utils.py:190]                                                                  Pclass_xf[0][0]                  \n",
      "I0220 22:15:09.562593 140492120463168 layer_utils.py:190]                                                                  Sex_xf[0][0]                     \n",
      "I0220 22:15:09.562661 140492120463168 layer_utils.py:190]                                                                  SibSp_xf[0][0]                   \n",
      "I0220 22:15:09.562735 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.563105 140492120463168 layer_utils.py:190] dense (Dense)                   (None, 1)            3           dense_features[0][0]             \n",
      "I0220 22:15:09.563227 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.563516 140492120463168 layer_utils.py:190] dense_1 (Dense)                 (None, 1)            2           dense[0][0]                      \n",
      "I0220 22:15:09.563610 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.563914 140492120463168 layer_utils.py:190] dense_features_1 (DenseFeatures (None, 3050)         0           Age_xf[0][0]                     \n",
      "I0220 22:15:09.564018 140492120463168 layer_utils.py:190]                                                                  Embarked_xf[0][0]                \n",
      "I0220 22:15:09.564103 140492120463168 layer_utils.py:190]                                                                  Fare_xf[0][0]                    \n",
      "I0220 22:15:09.564180 140492120463168 layer_utils.py:190]                                                                  Parch_xf[0][0]                   \n",
      "I0220 22:15:09.564248 140492120463168 layer_utils.py:190]                                                                  Pclass_xf[0][0]                  \n",
      "I0220 22:15:09.564325 140492120463168 layer_utils.py:190]                                                                  Sex_xf[0][0]                     \n",
      "I0220 22:15:09.564406 140492120463168 layer_utils.py:190]                                                                  SibSp_xf[0][0]                   \n",
      "I0220 22:15:09.564482 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.564647 140492120463168 layer_utils.py:190] concatenate (Concatenate)       (None, 3051)         0           dense_1[0][0]                    \n",
      "I0220 22:15:09.564725 140492120463168 layer_utils.py:190]                                                                  dense_features_1[0][0]           \n",
      "I0220 22:15:09.564798 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.565043 140492120463168 layer_utils.py:190] dense_2 (Dense)                 (None, 1)            3052        concatenate[0][0]                \n",
      "I0220 22:15:09.565129 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.565575 140492120463168 layer_utils.py:190] tf_op_layer_Squeeze (TensorFlow [(None,)]            0           dense_2[0][0]                    \n",
      "I0220 22:15:09.565683 140492120463168 layer_utils.py:257] ==================================================================================================\n",
      "I0220 22:15:09.566376 140492120463168 layer_utils.py:268] Total params: 3,057\n",
      "I0220 22:15:09.566506 140492120463168 layer_utils.py:269] Trainable params: 3,057\n",
      "I0220 22:15:09.566594 140492120463168 layer_utils.py:270] Non-trainable params: 0\n",
      "I0220 22:15:09.566657 140492120463168 layer_utils.py:271] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.733065 140492120463168 layer_utils.py:192] Model: \"functional_3\"\n",
      "I0220 22:15:09.733285 140492120463168 layer_utils.py:193] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.733404 140492120463168 layer_utils.py:190] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "I0220 22:15:09.733479 140492120463168 layer_utils.py:195] ==================================================================================================\n",
      "I0220 22:15:09.733738 140492120463168 layer_utils.py:190] Age_xf (InputLayer)             [(None,)]            0                                            \n",
      "I0220 22:15:09.733851 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.734308 140492120463168 layer_utils.py:190] Embarked_xf (InputLayer)        [(None,)]            0                                            \n",
      "I0220 22:15:09.734501 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.734793 140492120463168 layer_utils.py:190] Fare_xf (InputLayer)            [(None,)]            0                                            \n",
      "I0220 22:15:09.734889 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.735066 140492120463168 layer_utils.py:190] Parch_xf (InputLayer)           [(None,)]            0                                            \n",
      "I0220 22:15:09.735152 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.735354 140492120463168 layer_utils.py:190] Pclass_xf (InputLayer)          [(None,)]            0                                            \n",
      "I0220 22:15:09.735449 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.735633 140492120463168 layer_utils.py:190] Sex_xf (InputLayer)             [(None,)]            0                                            \n",
      "I0220 22:15:09.735826 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.736331 140492120463168 layer_utils.py:190] SibSp_xf (InputLayer)           [(None,)]            0                                            \n",
      "I0220 22:15:09.736485 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.736813 140492120463168 layer_utils.py:190] dense_features_2 (DenseFeatures (None, 2)            0           Age_xf[0][0]                     \n",
      "I0220 22:15:09.736914 140492120463168 layer_utils.py:190]                                                                  Embarked_xf[0][0]                \n",
      "I0220 22:15:09.737002 140492120463168 layer_utils.py:190]                                                                  Fare_xf[0][0]                    \n",
      "I0220 22:15:09.737075 140492120463168 layer_utils.py:190]                                                                  Parch_xf[0][0]                   \n",
      "I0220 22:15:09.737153 140492120463168 layer_utils.py:190]                                                                  Pclass_xf[0][0]                  \n",
      "I0220 22:15:09.737228 140492120463168 layer_utils.py:190]                                                                  Sex_xf[0][0]                     \n",
      "I0220 22:15:09.737302 140492120463168 layer_utils.py:190]                                                                  SibSp_xf[0][0]                   \n",
      "I0220 22:15:09.737422 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.737833 140492120463168 layer_utils.py:190] dense_3 (Dense)                 (None, 1)            3           dense_features_2[0][0]           \n",
      "I0220 22:15:09.737999 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.738363 140492120463168 layer_utils.py:190] dense_4 (Dense)                 (None, 1)            2           dense_3[0][0]                    \n",
      "I0220 22:15:09.738517 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.738852 140492120463168 layer_utils.py:190] dense_features_3 (DenseFeatures (None, 3050)         0           Age_xf[0][0]                     \n",
      "I0220 22:15:09.738962 140492120463168 layer_utils.py:190]                                                                  Embarked_xf[0][0]                \n",
      "I0220 22:15:09.739048 140492120463168 layer_utils.py:190]                                                                  Fare_xf[0][0]                    \n",
      "I0220 22:15:09.739120 140492120463168 layer_utils.py:190]                                                                  Parch_xf[0][0]                   \n",
      "I0220 22:15:09.739195 140492120463168 layer_utils.py:190]                                                                  Pclass_xf[0][0]                  \n",
      "I0220 22:15:09.739266 140492120463168 layer_utils.py:190]                                                                  Sex_xf[0][0]                     \n",
      "I0220 22:15:09.739342 140492120463168 layer_utils.py:190]                                                                  SibSp_xf[0][0]                   \n",
      "I0220 22:15:09.739414 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.739652 140492120463168 layer_utils.py:190] concatenate_1 (Concatenate)     (None, 3051)         0           dense_4[0][0]                    \n",
      "I0220 22:15:09.739739 140492120463168 layer_utils.py:190]                                                                  dense_features_3[0][0]           \n",
      "I0220 22:15:09.739825 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.740115 140492120463168 layer_utils.py:190] dense_5 (Dense)                 (None, 1)            3052        concatenate_1[0][0]              \n",
      "I0220 22:15:09.740216 140492120463168 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0220 22:15:09.740483 140492120463168 layer_utils.py:190] tf_op_layer_Squeeze_1 (TensorFl [(None,)]            0           dense_5[0][0]                    \n",
      "I0220 22:15:09.740581 140492120463168 layer_utils.py:257] ==================================================================================================\n",
      "I0220 22:15:09.741191 140492120463168 layer_utils.py:268] Total params: 3,057\n",
      "I0220 22:15:09.741291 140492120463168 layer_utils.py:269] Trainable params: 3,057\n",
      "I0220 22:15:09.741373 140492120463168 layer_utils.py:270] Non-trainable params: 0\n",
      "I0220 22:15:09.741447 140492120463168 layer_utils.py:271] __________________________________________________________________________________________________\n",
      "INFO:tensorflow:time(__main__.ModelTest.testBuildKerasModel): 0.51s\n",
      "I0220 22:15:09.742225 140492120463168 test_util.py:1973] time(__main__.ModelTest.testBuildKerasModel): 0.51s\n",
      "[       OK ] ModelTest.testBuildKerasModel\n",
      "[ RUN      ] ModelTest.test_session\n",
      "[  SKIPPED ] ModelTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.510s\n",
      "\n",
      "OK (skipped=1)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m models.features_test\n",
    "!{sys.executable} -m models.keras.model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tO9Jhplo0olX"
   },
   "source": [
    "## Step 4. Run your first TFX pipeline\n",
    "\n",
    "Components in the TFX pipeline will generate outputs for each run as [ML Metadata Artifacts](https://www.tensorflow.org/tfx/guide/mlmd), and they need to be stored somewhere. You can use any storage which the KFP cluster can access, and for this example we will use Google Cloud Storage (GCS). A default GCS bucket should have been created automatically. Its name will be `<your-project-id>-kubeflowpipelines-default`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "_data_root = os.path.join(\".\", 'data')\n",
    "_train_dirpath = os.path.join(_data_root, \"train\")\n",
    "_train_filepath = os.path.join(_train_dirpath, \"train.csv\")\n",
    "_test_dirpath = os.path.join(_data_root, \"test\")\n",
    "_test_filepath = os.path.join(_test_dirpath, \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zr-RjyPWTHdH"
   },
   "source": [
    "Let's upload our sample data to GCS bucket so that we can use it in our pipeline later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW-dSHW-TSdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading titanic.zip to ./data\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 11.9MB/s]\n",
      "Archive:  ./data/titanic.zip\n",
      "  inflating: ./data/gender_submission.csv  \n",
      "  inflating: ./data/test.csv         \n",
      "  inflating: ./data/train.csv        \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c titanic -p {_data_root} --force\n",
    "!unzip -o {_data_root}/\"titanic.zip\" -d {_data_root}\n",
    "!cp {_data_root}/\"train.csv\" {_train_filepath}\n",
    "!cp {_data_root}/\"test.csv\" {_test_filepath}\n",
    "\n",
    "# clean up\n",
    "!rm  {_data_root}/*.csv  {_data_root}/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy csv data to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/train/train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 59.8 KiB/ 59.8 KiB]                                                \n",
      "Operation completed over 1 objects/59.8 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/train/train.csv gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/titanic/data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc54hDZu0ole"
   },
   "source": [
    "Let's create a TFX pipeline using the `tfx pipeline create` command.\n",
    "\n",
    ">Note: When creating a pipeline for KFP, we need a container image which will be used to run our pipeline. And `skaffold` will build the image for us. Because skaffold pulls base images from the docker hub, it will take 5~10 minutes when we build the image for the first time, but it will take much less time from the second build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOU7zQof0olf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "No local setup.py, copying the directory and configuring the PYTHONPATH.\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/cloud-training-281409/titanic_pipeline4 -> gcr.io/cloud-training-281409/titanic_pipeline4:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/cloud-training-281409/titanic_pipeline4: Not found. Building\n",
      "[Skaffold] Building [gcr.io/cloud-training-281409/titanic_pipeline4]...\n",
      "[Skaffold] Sending build context to Docker daemon  1.116MB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.26.1\n",
      "[Skaffold]  ---> 6dd91a0791af\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 7ba72b899108\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 3c4da537067b\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 97eaba83dc33\n",
      "[Skaffold] Removing intermediate container 97eaba83dc33\n",
      "[Skaffold]  ---> 175e4b6d1923\n",
      "[Skaffold] Successfully built 175e4b6d1923\n",
      "[Skaffold] Successfully tagged gcr.io/cloud-training-281409/titanic_pipeline4:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/cloud-training-281409/titanic_pipeline4]\n",
      "[Skaffold] 0ddd9ba3a8e0: Preparing\n",
      "[Skaffold] 2c9c599e202d: Preparing\n",
      "[Skaffold] 1a67ae26cf47: Preparing\n",
      "[Skaffold] 25e69afdb83b: Preparing\n",
      "[Skaffold] 2bd41d6594e3: Preparing\n",
      "[Skaffold] 8e486d328b86: Preparing\n",
      "[Skaffold] 8f42d0a1a747: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 8e486d328b86: Waiting\n",
      "[Skaffold] 8f42d0a1a747: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] 2bd41d6594e3: Layer already exists\n",
      "[Skaffold] 25e69afdb83b: Layer already exists\n",
      "[Skaffold] 1a67ae26cf47: Layer already exists\n",
      "[Skaffold] 2c9c599e202d: Layer already exists\n",
      "[Skaffold] 8e486d328b86: Layer already exists\n",
      "[Skaffold] 8f42d0a1a747: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 0ddd9ba3a8e0: Pushed\n",
      "[Skaffold] latest: digest: sha256:024b9355f02295449dec849453c5be21a5399a0769ae7a8ece7f612e59f106b3 size: 3480\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:From /home/jupyter/ml-gcp-pipeline/titanic_pipeline/pipeline/pipeline.py:93: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Tuner\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: Tuner\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:   ->  Component: ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: Evaluator\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/ml-gcp-pipeline/titanic_pipeline/titanic_pipeline4.tar.gz\n",
      "{'created_at': datetime.datetime(2021, 2, 21, 0, 17, 40, tzinfo=tzlocal()),\n",
      " 'default_version': {'code_source_url': None,\n",
      "                     'created_at': datetime.datetime(2021, 2, 21, 0, 17, 40, tzinfo=tzlocal()),\n",
      "                     'id': 'b317a4c2-a3f5-4914-bdd9-f99aad0f1a37',\n",
      "                     'name': 'titanic_pipeline4',\n",
      "                     'package_url': None,\n",
      "                     'parameters': [{'name': 'pipeline-root',\n",
      "                                     'value': 'gs://cloud-training-281409-kubeflowpipelines-default/tfx_pipeline_output/titanic_pipeline4'}],\n",
      "                     'resource_references': [{'key': {'id': 'b317a4c2-a3f5-4914-bdd9-f99aad0f1a37',\n",
      "                                                      'type': 'PIPELINE'},\n",
      "                                              'name': None,\n",
      "                                              'relationship': 'OWNER'}]},\n",
      " 'description': None,\n",
      " 'error': None,\n",
      " 'id': 'b317a4c2-a3f5-4914-bdd9-f99aad0f1a37',\n",
      " 'name': 'titanic_pipeline4',\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://cloud-training-281409-kubeflowpipelines-default/tfx_pipeline_output/titanic_pipeline4'}],\n",
      " 'url': None}\n",
      "Please access the pipeline detail page at https://3b47c1a069ee9385-dot-us-west1.pipelines.googleusercontent.com/#/pipelines/details/b317a4c2-a3f5-4914-bdd9-f99aad0f1a37\n",
      "Pipeline \"titanic_pipeline4\" created successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline-path=kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build-target-image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmA6___Y0olh"
   },
   "source": [
    "While creating a pipeline, `Dockerfile` and `build.yaml` will be generated to build a Docker image. Don't forget to add these files to the source control system (for example, git) along with other source files.\n",
    "\n",
    "A pipeline definition file for [argo](https://argoproj.github.io/argo/) will be generated, too. The name of this file is `${PIPELINE_NAME}.tar.gz`. For example, it will be `my_pipeline.tar.gz` if the name of your pipeline is `my_pipeline`. It is recommended NOT to include this pipeline definition file into source control, because it will be generated from other Python files and will be updated whenever you update the pipeline. For your convenience, this file is already listed in `.gitignore` which is generated automatically.\n",
    "\n",
    "NOTE: `kubeflow` will be automatically selected as an orchestration engine if `airflow` is not installed and `--engine` is not specified.\n",
    "\n",
    "Now start an execution run with the newly created pipeline using the `tfx run create` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKSjVVsa0oli"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating a run for pipeline: titanic_pipeline4\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: titanic_pipeline4\n",
      "+-------------------+--------------------------------------+----------+---------------------------+---------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name     | run_id                               | status   | created_at                | link                                                                                                                      |\n",
      "+===================+======================================+==========+===========================+===========================================================================================================================+\n",
      "| titanic_pipeline4 | 431ce8f1-0162-4aad-9b30-67473ac2bc29 |          | 2021-02-21T00:18:00+00:00 | https://3b47c1a069ee9385-dot-us-west1.pipelines.googleusercontent.com/#/runs/details/431ce8f1-0162-4aad-9b30-67473ac2bc29 |\n",
      "+-------------------+--------------------------------------+----------+---------------------------+---------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pg0VxvUC0olk"
   },
   "source": [
    "Or, you can also run the pipeline in the KFP Dashboard.  The new execution run will be listed under Experiments in the KFP Dashboard.  Clicking into the experiment will allow you to monitor progress and visualize the artifacts created during the execution run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLN4ges90oll"
   },
   "source": [
    "However, we recommend visiting the KFP Dashboard. You can access the KFP Dashboard from the Cloud AI Platform Pipelines menu in Google Cloud Console. Once you visit the dashboard, you will be able to find the pipeline, and access a wealth of information about the pipeline.\n",
    "For example, you can find your runs under the *Experiments* menu, and when you open your execution run under Experiments you can find all your artifacts from the pipeline under *Artifacts* menu.\n",
    "\n",
    ">Note: If your pipeline run fails, you can see detailed logs for each TFX component in the Experiments tab in the KFP Dashboard.\n",
    "    \n",
    "One of the major sources of failure is permission related problems. Please make sure your KFP cluster has permissions to access Google Cloud APIs. This can be configured [when you create a KFP cluster in GCP](https://cloud.google.com/ai-platform/pipelines/docs/setting-up), or see [Troubleshooting document in GCP](https://cloud.google.com/ai-platform/pipelines/docs/troubleshooting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create local pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline-path=local_runner.py \\\n",
    "--build-target-image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run local pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python local_runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE-Pqvto0olm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:From /home/jupyter/ml-gcp-pipeline/titanic_pipeline/pipeline/pipeline.py:93: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Evaluator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/ml-gcp-pipeline/titanic_pipeline/titanic_pipeline2.tar.gz\n",
      "{'code_source_url': None,\n",
      " 'created_at': datetime.datetime(2021, 2, 20, 23, 37, 2, tzinfo=tzlocal()),\n",
      " 'id': 'e7717d91-4331-476b-8d68-38ce8bc76ce4',\n",
      " 'name': 'titanic_pipeline2_20210220233702',\n",
      " 'package_url': None,\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://cloud-training-281409-kubeflowpipelines-default/tfx_pipeline_output/titanic_pipeline2'}],\n",
      " 'resource_references': [{'key': {'id': 'eb1c4b47-04a8-4e0f-914f-8684a5f77e0c',\n",
      "                                  'type': 'PIPELINE'},\n",
      "                          'name': None,\n",
      "                          'relationship': 'OWNER'}]}\n",
      "Please access the pipeline detail page at https://3b47c1a069ee9385-dot-us-west1.pipelines.googleusercontent.com/#/pipelines/details/eb1c4b47-04a8-4e0f-914f-8684a5f77e0c\n",
      "Pipeline \"titanic_pipeline2\" updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Update the pipeline\n",
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "# You can run the pipeline the same way.\n",
    "#!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8q1ZYEHX0olo"
   },
   "source": [
    "### Check pipeline outputs\n",
    "\n",
    "Visit the KFP dashboard to find pipeline outputs in the page for your pipeline run. Click the *Experiments* tab on the left, and *All runs* in the Experiments page. You should be able to find the latest run under the name of your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWMBXU510olp"
   },
   "source": [
    "## Step 6. Add components for training.\n",
    "\n",
    "In this step, you will add components for training and model validation including `Transform`, `Trainer`, `ResolverNode`, `Evaluator`, and `Pusher`.\n",
    "\n",
    ">**Double-click to open `pipeline.py`**. Find and uncomment the 5 lines which add `Transform`, `Trainer`, `ResolverNode`, `Evaluator` and `Pusher` to the pipeline. (Tip: search for `TODO(step 6):`)\n",
    "\n",
    "As you did before, you now need to update the existing pipeline with the modified pipeline definition. The instructions are the same as Step 5. Update the pipeline using `tfx pipeline update`, and create an execution run using `tfx run create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQDNitkH0olq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/cloud-training-281409/titanic-pipeline -> gcr.io/cloud-training-281409/titanic-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/cloud-training-281409/titanic-pipeline: Not found. Building\n",
      "[Skaffold] Building [gcr.io/cloud-training-281409/titanic-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  1.161MB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.26.1\n",
      "[Skaffold]  ---> 6dd91a0791af\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> a0efa056cb46\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 583ab8d30268\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 55b6d9177100\n",
      "[Skaffold] Removing intermediate container 55b6d9177100\n",
      "[Skaffold]  ---> ef7781c9c0d1\n",
      "[Skaffold] Successfully built ef7781c9c0d1\n",
      "[Skaffold] Successfully tagged gcr.io/cloud-training-281409/titanic-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/cloud-training-281409/titanic-pipeline]\n",
      "[Skaffold] 930044a8070d: Preparing\n",
      "[Skaffold] 0fdd55dd43fc: Preparing\n",
      "[Skaffold] 1a67ae26cf47: Preparing\n",
      "[Skaffold] 25e69afdb83b: Preparing\n",
      "[Skaffold] 2bd41d6594e3: Preparing\n",
      "[Skaffold] 8e486d328b86: Preparing\n",
      "[Skaffold] 8f42d0a1a747: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 8e486d328b86: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 8f42d0a1a747: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 2bd41d6594e3: Layer already exists\n",
      "[Skaffold] 25e69afdb83b: Layer already exists\n",
      "[Skaffold] 1a67ae26cf47: Layer already exists\n",
      "[Skaffold] 0fdd55dd43fc: Layer already exists\n",
      "[Skaffold] 8e486d328b86: Layer already exists\n",
      "[Skaffold] 8f42d0a1a747: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 930044a8070d: Pushed\n",
      "[Skaffold] latest: digest: sha256:bf304177150f430123d395a0591e2c8379dc1484c3518c0e870ad4174a2bcf34 size: 3480\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:From /home/jupyter/ml-gcp-pipeline/titanic_pipeline/pipeline/pipeline.py:78: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Evaluator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/ml-gcp-pipeline/titanic_pipeline/titanic_pipeline.tar.gz\n",
      "{'code_source_url': None,\n",
      " 'created_at': datetime.datetime(2021, 2, 17, 17, 13, 27, tzinfo=tzlocal()),\n",
      " 'id': '0a60cc61-493d-4c46-be19-603a6a58e8a9',\n",
      " 'name': 'titanic_pipeline_20210217171326',\n",
      " 'package_url': None,\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://cloud-training-281409-kubeflowpipelines-default/tfx_pipeline_output/titanic_pipeline'}],\n",
      " 'resource_references': [{'key': {'id': '5aaac790-bea2-4661-b5d8-8f21e3e87b27',\n",
      "                                  'type': 'PIPELINE'},\n",
      "                          'name': None,\n",
      "                          'relationship': 'OWNER'}]}\n",
      "Please access the pipeline detail page at https://4305b149447231cf-dot-us-west1.pipelines.googleusercontent.com/#/pipelines/details/5aaac790-bea2-4661-b5d8-8f21e3e87b27\n",
      "Pipeline \"titanic_pipeline\" updated successfully.\n",
      "CLI\n",
      "Creating a run for pipeline: titanic_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: titanic_pipeline\n",
      "+------------------+--------------------------------------+----------+---------------------------+---------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name    | run_id                               | status   | created_at                | link                                                                                                                      |\n",
      "+==================+======================================+==========+===========================+===========================================================================================================================+\n",
      "| titanic_pipeline | e6d71d62-e4e5-4a31-82c7-ed20ee104758 |          | 2021-02-17T17:13:33+00:00 | https://4305b149447231cf-dot-us-west1.pipelines.googleusercontent.com/#/runs/details/e6d71d62-e4e5-4a31-82c7-ed20ee104758 |\n",
      "+------------------+--------------------------------------+----------+---------------------------+---------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksWfVQUnMYCX"
   },
   "source": [
    "When this execution run finishes successfully, you have now created and run your first TFX pipeline in AI Platform Pipelines!\n",
    "\n",
    "**NOTE:** You might have noticed that every time we create a pipeline run, every component runs again and again even though the input and the parameters were not changed.\n",
    "It is waste of time and resources, and you can skip those executions with pipeline caching. You can enable caching by specifying `enable_cache=True` for the `Pipeline` object in `pipeline.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkF7klWi0ols"
   },
   "source": [
    "## Step 7. (*Optional*) Try BigQueryExampleGen\n",
    "\n",
    "[BigQuery](https://cloud.google.com/bigquery) is a serverless, highly scalable, and cost-effective cloud data warehouse. BigQuery can be used as a source for training examples in TFX. In this step, we will add `BigQueryExampleGen` to the pipeline.\n",
    "\n",
    ">**Double-click to open `pipeline.py`**. Comment out `CsvExampleGen` and uncomment the line which creates an instance of `BigQueryExampleGen`. You also need to uncomment the `query` argument of the `create_pipeline` function.\n",
    "\n",
    "We need to specify which GCP project to use for BigQuery, and this is done by setting `--project` in `beam_pipeline_args` when creating a pipeline.\n",
    "\n",
    ">**Double-click to open `configs.py`**. Uncomment the definition of `GOOGLE_CLOUD_REGION`, `BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS` and `BIG_QUERY_QUERY`. You should replace the region value in this file with the correct values for your GCP project.\n",
    "\n",
    ">**Note: You MUST set your GCP region in the `configs.py` file before proceeding.**\n",
    "\n",
    ">**Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is `my_pipeline` if you didn't change.\n",
    "\n",
    ">**Double-click to open `kubeflow_runner.py`**. Uncomment two arguments, `query` and `beam_pipeline_args`, for the `create_pipeline` function.\n",
    "\n",
    "Now the pipeline is ready to use BigQuery as an example source. Update the pipeline as before and create a new execution run as we did in step 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1sD3NxB60olt"
   },
   "outputs": [],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LpA2R6Lu0olv"
   },
   "source": [
    "## Step 8. (*Optional*) Try Dataflow with KFP\n",
    "\n",
    "Several [TFX Components uses Apache Beam](https://www.tensorflow.org/tfx/guide/beam) to implement data-parallel pipelines, and it means that you can distribute data processing workloads using [Google Cloud Dataflow](https://cloud.google.com/dataflow/). In this step, we will set the Kubeflow orchestrator to use dataflow as the data processing back-end for Apache Beam.\n",
    "\n",
    ">**Double-click `pipeline` to change directory, and double-click to open `configs.py`**. Uncomment the definition of `GOOGLE_CLOUD_REGION`, and `DATAFLOW_BEAM_PIPELINE_ARGS`.\n",
    "\n",
    ">**Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is `my_pipeline` if you didn't change.\n",
    "\n",
    ">**Double-click to open `kubeflow_runner.py`**. Uncomment `beam_pipeline_args`. (Also make sure to comment out current `beam_pipeline_args` that you added in Step 7.)\n",
    "\n",
    "Now the pipeline is ready to use Dataflow. Update the pipeline and create an execution run as we did in step 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3HVPcKi0olw"
   },
   "outputs": [],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uxDY13N0oly"
   },
   "source": [
    "You can find your Dataflow jobs in [Dataflow in Cloud Console](http://console.cloud.google.com/dataflow).\n",
    "\n",
    ">**Double-click to open `pipeline.py`**. Reset the value of `enable_cache` to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxOjhBmG0ol0"
   },
   "outputs": [],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkoIMUfj0ol2"
   },
   "source": [
    "You can find your training jobs in [Cloud AI Platform Jobs](https://console.cloud.google.com/ai-platform/jobs). If your pipeline completed successfully, you can find your model in [Cloud AI Platform Models](https://console.cloud.google.com/ai-platform/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DRTFdTy0ol3"
   },
   "source": [
    "## Step 10. Ingest YOUR data to the pipeline\n",
    "\n",
    "We made a pipeline for a model using the Chicago Taxi dataset. Now it's time to put your data into the pipeline.\n",
    "\n",
    "Your data can be stored anywhere your pipeline can access, including GCS, or BigQuery. You will need to modify the pipeline definition to access your data.\n",
    "\n",
    "1. If your data is stored in files, modify the `DATA_PATH` in `kubeflow_runner.py` or `local_runner.py` and set it to the location of your files. If your data is stored in BigQuery, modify `BIG_QUERY_QUERY` in `pipeline/configs.py` to correctly query for your data.\n",
    "1. Add features in `models/features.py`.\n",
    "1. Modify `models/preprocessing.py` to [transform input data for training](https://www.tensorflow.org/tfx/guide/transform).\n",
    "1. Modify `models/keras/model.py` and `models/keras/constants.py` to [describe your ML model](https://www.tensorflow.org/tfx/guide/trainer).\n",
    "  - You can use an estimator based model, too. Change `RUN_FN` constant to `models.estimator.model.run_fn` in `pipeline/configs.py`.\n",
    "\n",
    "Please see [Trainer component guide](https://www.tensorflow.org/tfx/guide/trainer) for more introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20KRGsPX0ol3"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Alternatively, you can clean up individual resources by visiting each consoles:\n",
    "- [Google Cloud Storage](https://console.cloud.google.com/storage)\n",
    "- [Google Container Registry](https://console.cloud.google.com/gcr)\n",
    "- [Google Kubernetes Engine](https://console.cloud.google.com/kubernetes)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
