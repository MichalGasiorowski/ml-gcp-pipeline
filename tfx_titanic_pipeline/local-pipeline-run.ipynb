{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training with TFX and Google Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1.  Use the TFX CLI to build a TFX pipeline.\n",
    "2.  Deploy a TFX pipeline version without tuning to a hosted AI Platform Pipelines instance.\n",
    "3.  Create and monitor a TFX pipeline run using the TFX CLI.\n",
    "4.  Deploy a new TFX pipeline version with tuning enabled to a hosted AI Platform Pipelines instance.\n",
    "5.  Create and monitor another TFX pipeline run directly in the KFP UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you use utilize the following tools and services to deploy and run a TFX pipeline on Google Cloud that automates the development and deployment of a TensorFlow 2.3 WideDeep Classifer to predict forest cover from cartographic data:\n",
    "\n",
    "* The [**TFX CLI**](https://www.tensorflow.org/tfx/guide/cli) utility to build and deploy a TFX pipeline.\n",
    "* A hosted [**AI Platform Pipeline instance (Kubeflow Pipelines)**](https://www.tensorflow.org/tfx/guide/kubeflow) for TFX pipeline orchestration.\n",
    "* [**Dataflow**](https://cloud.google.com/dataflow) jobs for scalable, distributed data processing for TFX components.\n",
    "* A [**AI Platform Training**](https://cloud.google.com/ai-platform/) job for model training and flock management for parallel tuning trials. \n",
    "* [**AI Platform Prediction**](https://cloud.google.com/ai-platform/) as a model server destination for blessed pipeline model versions.\n",
    "* [**CloudTuner**](https://www.tensorflow.org/tfx/guide/tuner#tuning_on_google_cloud_platform_gcp) and [**AI Platform Vizier**](https://cloud.google.com/ai-platform/optimizer/docs/overview) for advanced model hyperparameter tuning using the Vizier algorithm.\n",
    "\n",
    "You will then create and monitor pipeline runs using the TFX CLI as well as the KFP UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update lab environment PATH to include TFX CLI and skaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/michal/.local/bin:/home/michal/venv/ML-3.8/bin:/home/michal/google-cloud-sdk/bin:/home/michal/anaconda3/bin:/home/michal/anaconda3/condabin:/home/michal/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Set `PATH` to include the directory containing TFX CLI and skaffold.\n",
    "PATH=%env PATH\n",
    "HOME=%env HOME\n",
    "\n",
    "%env PATH={HOME}/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate lab package version installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.25.0\n",
      "KFP version: 1.0.4\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\"\n",
    "!python -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this lab was built and tested with the following package versions:\n",
    "\n",
    "`TFX version: 0.25.0`  \n",
    "`KFP version: 1.0.4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup local path to data, train, test folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_path=os.getcwd()\n",
    "local_data_dirpath = os.path.join(notebook_path, 'data')\n",
    "\n",
    "local_train_dirpath = os.path.join(local_data_dirpath, \"train\")\n",
    "local_train_filepath = os.path.join(local_train_dirpath, \"train.csv\")\n",
    "local_test_dirpath = os.path.join(local_data_dirpath, \"test\")\n",
    "local_test_filepath = os.path.join(local_test_dirpath, \"test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!rm kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from kaggle, unzip it and copy it to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c titanic -p {local_data_dirpath} --force\n",
    "!unzip -o {local_data_dirpath}/\"titanic.zip\" -d {local_data_dirpath}\n",
    "!cp {local_data_dirpath}/\"train.csv\" {local_train_filepath}\n",
    "!cp {local_data_dirpath}/\"test.csv\" {local_test_filepath}\n",
    "\n",
    "# clean up\n",
    "!rm  {local_data_dirpath}/*.csv  {local_data_dirpath}/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy data to gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!gsutil cp data/train/train.csv gs://cloud-training-281409-kubeflowpipelines-default/tfx-template/data/titanic/data.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = 'tfx-titanic-training'\n",
    "MODEL_NAME = 'tfx_titanic_classifier'\n",
    "DATA_ROOT_URI = local_train_dirpath\n",
    "RUNTIME_VERSION = '2.3'\n",
    "PYTHON_VERSION = '3.7'\n",
    "ENABLE_TUNING=False\n",
    "ENABLE_CACHE=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_NAME=tfx-titanic-training\n",
      "env: MODEL_NAME=tfx_titanic_classifier\n",
      "env: DATA_ROOT_URI=/home/michal/PycharmProjects/ml-gcp-pipeline/tfx_titanic_pipeline/data/train\n",
      "env: RUNTIME_VERSION=2.3\n",
      "env: PYTHON_VERSION=3.7\n",
      "env: ENABLE_TUNING=False\n",
      "env: ENABLE_CACHE=True\n"
     ]
    }
   ],
   "source": [
    "%env PIPELINE_NAME={PIPELINE_NAME}\n",
    "%env MODEL_NAME={MODEL_NAME}\n",
    "%env DATA_ROOT_URI={DATA_ROOT_URI}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}\n",
    "%env ENABLE_TUNING={ENABLE_TUNING}\n",
    "%env ENABLE_CACHE={ENABLE_CACHE}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michal/PycharmProjects/ml-gcp-pipeline/tfx_titanic_pipeline/pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd {notebook_path}/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 23:14:01.412088: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:PIPELINE_ROOT=/home/michal/artifact-store/tfx-titanic-training/20210408_231408\n",
      "INFO:absl:Cleaning local log folder : /tmp/logs\n",
      "INFO:absl:train_steps for training: 30000\n",
      "INFO:absl:tuner_steps for tuning: 2000\n",
      "INFO:absl:data_root_uri for training: /home/michal/PycharmProjects/ml-gcp-pipeline/tfx_titanic_pipeline/data/train\n",
      "INFO:absl:eval_steps for evaluating: 1000\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running driver for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:Running executor for CsvExampleGen\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data /home/michal/PycharmProjects/ml-gcp-pipeline/tfx_titanic_pipeline/data/train/* to TFExample.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Running publisher for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "INFO:absl:Component ImporterNode.import_user_schema is running.\n",
      "INFO:absl:Running driver for ImporterNode.import_user_schema\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Processing source uri: schema, properties: {}, custom_properties: {}\n",
      "2021-04-08 23:14:09.868347: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "INFO:absl:Running executor for ImporterNode.import_user_schema\n",
      "INFO:absl:Running publisher for ImporterNode.import_user_schema\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component ImporterNode.import_user_schema is finished.\n",
      "INFO:absl:Component ResolverNode.latest_blessed_model_resolver is running.\n",
      "INFO:absl:Running driver for ResolverNode.latest_blessed_model_resolver\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Running publisher for ResolverNode.latest_blessed_model_resolver\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component ResolverNode.latest_blessed_model_resolver is finished.\n",
      "INFO:absl:Component StatisticsGen is running.\n",
      "INFO:absl:Running driver for StatisticsGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Running executor for StatisticsGen\n",
      "INFO:absl:Generating statistics for split train.\n",
      "INFO:absl:Statistics for split train written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/StatisticsGen/statistics/4/train.\n",
      "INFO:absl:Generating statistics for split eval.\n",
      "INFO:absl:Statistics for split eval written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/StatisticsGen/statistics/4/eval.\n",
      "2021-04-08 23:14:11.006399: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2021-04-08 23:14:11.140186: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-04-08 23:14:11.140239: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (michal-ubuntu20-vm): /proc/driver/nvidia/version does not exist\n",
      "2021-04-08 23:14:11.140508: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-04-08 23:14:11.164789: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 4008000000 Hz\n",
      "2021-04-08 23:14:11.165657: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x63898c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-04-08 23:14:11.165696: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "INFO:absl:Running publisher for StatisticsGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component StatisticsGen is finished.\n",
      "INFO:absl:Component Transform is running.\n",
      "INFO:absl:Running driver for Transform\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "2021-04-08 23:14:12.284652: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "2021-04-08 23:14:12.287267: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "INFO:absl:Running executor for Transform\n",
      "INFO:absl:Analyze the 'train' split and transform all splits when splits_config is not set.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tfx/components/transform/executor.py:528: Schema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Schema is a deprecated, use schema_utils.schema_from_feature_spec to create a `Schema`\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tfx/components/transform/executor.py:528: Schema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Schema is a deprecated, use schema_utils.schema_from_feature_spec to create a `Schema`\n",
      "INFO:absl:Feature Embarked has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Ticket has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Sex has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Name has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Cabin has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Age has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Fare has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Parch has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature PassengerId has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Pclass has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature SibSp has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Survived has no shape. Setting to VarLenSparseTensor.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow_transform/tf_utils.py:250: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow_transform/tf_utils.py:250: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "INFO:absl:Feature Embarked has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Ticket has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Sex has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Name has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Cabin has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Age has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Fare has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Parch has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature PassengerId has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Pclass has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature SibSp has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Survived has no shape. Setting to VarLenSparseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Feature Embarked has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Ticket has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Sex has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Name has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Cabin has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Age has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Fare has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Parch has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature PassengerId has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Pclass has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature SibSp has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Survived has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Embarked has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Ticket has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Sex has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Name has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Cabin has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Age has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Fare has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Parch has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature PassengerId has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Pclass has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature SibSp has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Survived has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Embarked has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Ticket has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Sex has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Name has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Cabin has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Age has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Fare has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Parch has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature PassengerId has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Pclass has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature SibSp has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Survived has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Embarked has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Ticket has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Sex has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Name has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Cabin has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Age has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Fare has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Parch has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature PassengerId has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Pclass has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature SibSp has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Survived has no shape. Setting to VarLenSparseTensor.\n",
      "WARNING:tensorflow:TFT beam APIs accept both the TFXIO format and the instance dict format now. There is no need to set use_tfxio any more and it will be removed soon.\n",
      "WARNING:tensorflow:TFT beam APIs accept both the TFXIO format and the instance dict format now. There is no need to set use_tfxio any more and it will be removed soon.\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:200: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:200: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "INFO:absl:Feature Embarked has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Ticket has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Sex has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Name has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Cabin has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Age has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Fare has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Parch has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature PassengerId has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Pclass has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature SibSp has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Survived has no shape. Setting to VarLenSparseTensor.\n",
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
      "INFO:absl:Feature Embarked has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Ticket has no shape. Setting to VarLenSparseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Feature Sex has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Name has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Cabin has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Age has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Fare has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Parch has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature PassengerId has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Pclass has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature SibSp has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature Survived has no shape. Setting to VarLenSparseTensor.\n",
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "WARNING:tensorflow:Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring send_type hint: <class 'NoneType'>\n",
      "WARNING:apache_beam.typehints.typehints:Ignoring return_type hint: <class 'NoneType'>\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_3:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
      "\n",
      "INFO:absl:Running publisher for Transform\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Transform is finished.\n",
      "INFO:absl:Component ExampleValidator is running.\n",
      "INFO:absl:Running driver for ExampleValidator\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Running executor for ExampleValidator\n",
      "INFO:absl:Validating schema against the computed statistics for split train.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow_data_validation/utils/stats_util.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow_data_validation/utils/stats_util.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:absl:Validation complete for split train. Anomalies written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/ExampleValidator/anomalies/6/train.\n",
      "INFO:absl:Validating schema against the computed statistics for split eval.\n",
      "INFO:absl:Validation complete for split eval. Anomalies written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/ExampleValidator/anomalies/6/eval.\n",
      "INFO:absl:Running publisher for ExampleValidator\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component ExampleValidator is finished.\n",
      "INFO:absl:Component SchemaGen is running.\n",
      "INFO:absl:Running driver for SchemaGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Running executor for SchemaGen\n",
      "INFO:absl:Processing schema from statistics for split train.\n",
      "INFO:absl:Processing schema from statistics for split eval.\n",
      "INFO:absl:Schema written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/SchemaGen/schema/7/schema.pbtxt.\n",
      "INFO:absl:Running publisher for SchemaGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component SchemaGen is finished.\n",
      "INFO:absl:Component Trainer is running.\n",
      "INFO:absl:Running driver for Trainer\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "2021-04-08 23:14:18.558141: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "2021-04-08 23:14:18.560541: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "INFO:absl:Running executor for Trainer\n",
      "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
      "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "INFO:absl:Training model.\n",
      "INFO:absl:Feature Age_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Embarked_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Fare_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Parch_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Pclass_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Sex_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature SibSp_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Survived_xf has a shape . Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Feature Age_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Embarked_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Fare_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Parch_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Pclass_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Sex_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature SibSp_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature Survived_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:HyperParameters for training: {'space': [{'class_name': 'Float', 'config': {'name': 'learning_rate', 'default': 0.0009167702421017742, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}}, {'class_name': 'Int', 'config': {'name': 'n_layers', 'default': 2, 'conditions': [], 'min_value': 1, 'max_value': 2, 'step': 1, 'sampling': None}}, {'class_name': 'Int', 'config': {'name': 'n_units_1', 'default': 72, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'n_layers', 'values': [1]}}], 'min_value': 8, 'max_value': 128, 'step': 8, 'sampling': None}}, {'class_name': 'Int', 'config': {'name': 'n_units_1', 'default': 128, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'n_layers', 'values': [2]}}], 'min_value': 8, 'max_value': 128, 'step': 8, 'sampling': None}}, {'class_name': 'Int', 'config': {'name': 'n_units_2', 'default': 80, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'n_layers', 'values': [2]}}], 'min_value': 8, 'max_value': 128, 'step': 8, 'sampling': None}}], 'values': {'learning_rate': 0.0009167702421017742, 'n_layers': 2, 'n_units_1': 128, 'n_units_2': 80}}\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:absl:Model: \"functional_1\"\n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl:Age_xf (InputLayer)             [(None,)]            0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:Embarked_xf (InputLayer)        [(None,)]            0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:Fare_xf (InputLayer)            [(None,)]            0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:Parch_xf (InputLayer)           [(None,)]            0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:Pclass_xf (InputLayer)          [(None,)]            0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:Sex_xf (InputLayer)             [(None,)]            0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:SibSp_xf (InputLayer)           [(None,)]            0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:dense_features (DenseFeatures)  (None, 2)            0           Age_xf[0][0]                     \n",
      "INFO:absl:                                                                 Embarked_xf[0][0]                \n",
      "INFO:absl:                                                                 Fare_xf[0][0]                    \n",
      "INFO:absl:                                                                 Parch_xf[0][0]                   \n",
      "INFO:absl:                                                                 Pclass_xf[0][0]                  \n",
      "INFO:absl:                                                                 Sex_xf[0][0]                     \n",
      "INFO:absl:                                                                 SibSp_xf[0][0]                   \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:dense (Dense)                   (None, 128)          384         dense_features[0][0]             \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:dense_1 (Dense)                 (None, 80)           10320       dense[0][0]                      \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:dense_features_1 (DenseFeatures (None, 3050)         0           Age_xf[0][0]                     \n",
      "INFO:absl:                                                                 Embarked_xf[0][0]                \n",
      "INFO:absl:                                                                 Fare_xf[0][0]                    \n",
      "INFO:absl:                                                                 Parch_xf[0][0]                   \n",
      "INFO:absl:                                                                 Pclass_xf[0][0]                  \n",
      "INFO:absl:                                                                 Sex_xf[0][0]                     \n",
      "INFO:absl:                                                                 SibSp_xf[0][0]                   \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:concatenate (Concatenate)       (None, 3130)         0           dense_1[0][0]                    \n",
      "INFO:absl:                                                                 dense_features_1[0][0]           \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:dense_2 (Dense)                 (None, 1)            3131        concatenate[0][0]                \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:tf_op_layer_Squeeze (TensorFlow [(None,)]            0           dense_2[0][0]                    \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl:Total params: 13,835\n",
      "INFO:absl:Trainable params: 13,835\n",
      "INFO:absl:Non-trainable params: 0\n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "2021-04-08 23:14:19.092042: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "Epoch 1/10\n",
      "2021-04-08 23:14:21.506170: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2021-04-08 23:14:21.528895: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: /tmp/logs/train/plugins/profile/2021_04_08_23_14_21\n",
      "2021-04-08 23:14:21.530948: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to /tmp/logs/train/plugins/profile/2021_04_08_23_14_21/michal-ubuntu20-vm.trace.json.gz\n",
      "2021-04-08 23:14:21.539695: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: /tmp/logs/train/plugins/profile/2021_04_08_23_14_21\n",
      "2021-04-08 23:14:21.539830: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to /tmp/logs/train/plugins/profile/2021_04_08_23_14_21/michal-ubuntu20-vm.memory_profile.json.gz\n",
      "2021-04-08 23:14:21.540995: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: /tmp/logs/train/plugins/profile/2021_04_08_23_14_21Dumped tool data for xplane.pb to /tmp/logs/train/plugins/profile/2021_04_08_23_14_21/michal-ubuntu20-vm.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /tmp/logs/train/plugins/profile/2021_04_08_23_14_21/michal-ubuntu20-vm.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /tmp/logs/train/plugins/profile/2021_04_08_23_14_21/michal-ubuntu20-vm.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /tmp/logs/train/plugins/profile/2021_04_08_23_14_21/michal-ubuntu20-vm.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /tmp/logs/train/plugins/profile/2021_04_08_23_14_21/michal-ubuntu20-vm.kernel_stats.pb\n",
      "\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.0263s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.0263s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 - 12s - loss: 0.4900 - tp: 40974.0000 - fp: 11485.0000 - tn: 109672.0000 - fn: 29869.0000 - binary_accuracy: 0.7846 - precision: 0.7811 - recall: 0.5784 - auc: 0.8219 - val_loss: 0.4260 - val_tp: 21841.0000 - val_fp: 5649.0000 - val_tn: 29734.0000 - val_fn: 6776.0000 - val_binary_accuracy: 0.8059 - val_precision: 0.7945 - val_recall: 0.7632 - val_auc: 0.8888\n",
      "Epoch 2/10\n",
      "3000/3000 - 14s - loss: 0.4575 - tp: 47365.0000 - fp: 15980.0000 - tn: 105188.0000 - fn: 23467.0000 - binary_accuracy: 0.7945 - precision: 0.7477 - recall: 0.6687 - auc: 0.8365 - val_loss: 0.4246 - val_tp: 21461.0000 - val_fp: 6022.0000 - val_tn: 29365.0000 - val_fn: 7152.0000 - val_binary_accuracy: 0.7942 - val_precision: 0.7809 - val_recall: 0.7500 - val_auc: 0.8856\n",
      "Epoch 3/10\n",
      "3000/3000 - 18s - loss: 0.4571 - tp: 47396.0000 - fp: 15989.0000 - tn: 105182.0000 - fn: 23433.0000 - binary_accuracy: 0.7947 - precision: 0.7477 - recall: 0.6692 - auc: 0.8373 - val_loss: 0.4236 - val_tp: 21461.0000 - val_fp: 6024.0000 - val_tn: 29364.0000 - val_fn: 7151.0000 - val_binary_accuracy: 0.7941 - val_precision: 0.7808 - val_recall: 0.7501 - val_auc: 0.8860\n",
      "Epoch 4/10\n",
      "3000/3000 - 18s - loss: 0.4571 - tp: 47411.0000 - fp: 15975.0000 - tn: 105187.0000 - fn: 23427.0000 - binary_accuracy: 0.7948 - precision: 0.7480 - recall: 0.6693 - auc: 0.8374 - val_loss: 0.4237 - val_tp: 21459.0000 - val_fp: 6022.0000 - val_tn: 29363.0000 - val_fn: 7156.0000 - val_binary_accuracy: 0.7941 - val_precision: 0.7809 - val_recall: 0.7499 - val_auc: 0.8864\n",
      "Epoch 5/10\n",
      "3000/3000 - 17s - loss: 0.4571 - tp: 47397.0000 - fp: 15989.0000 - tn: 105188.0000 - fn: 23426.0000 - binary_accuracy: 0.7947 - precision: 0.7478 - recall: 0.6692 - auc: 0.8374 - val_loss: 0.4236 - val_tp: 21466.0000 - val_fp: 6020.0000 - val_tn: 29362.0000 - val_fn: 7152.0000 - val_binary_accuracy: 0.7942 - val_precision: 0.7810 - val_recall: 0.7501 - val_auc: 0.8864\n",
      "Epoch 6/10\n",
      "3000/3000 - 17s - loss: 0.4571 - tp: 47401.0000 - fp: 15978.0000 - tn: 105172.0000 - fn: 23449.0000 - binary_accuracy: 0.7947 - precision: 0.7479 - recall: 0.6690 - auc: 0.8375 - val_loss: 0.4235 - val_tp: 21457.0000 - val_fp: 6024.0000 - val_tn: 29369.0000 - val_fn: 7150.0000 - val_binary_accuracy: 0.7942 - val_precision: 0.7808 - val_recall: 0.7501 - val_auc: 0.8860\n",
      "Epoch 7/10\n",
      "3000/3000 - 17s - loss: 0.4570 - tp: 47399.0000 - fp: 15976.0000 - tn: 105196.0000 - fn: 23429.0000 - binary_accuracy: 0.7948 - precision: 0.7479 - recall: 0.6692 - auc: 0.8375 - val_loss: 0.4240 - val_tp: 21456.0000 - val_fp: 6022.0000 - val_tn: 29369.0000 - val_fn: 7153.0000 - val_binary_accuracy: 0.7941 - val_precision: 0.7808 - val_recall: 0.7500 - val_auc: 0.8860\n",
      "Epoch 8/10\n",
      "3000/3000 - 15s - loss: 0.4571 - tp: 47395.0000 - fp: 15986.0000 - tn: 105179.0000 - fn: 23440.0000 - binary_accuracy: 0.7947 - precision: 0.7478 - recall: 0.6691 - auc: 0.8375 - val_loss: 0.4234 - val_tp: 21462.0000 - val_fp: 6021.0000 - val_tn: 29361.0000 - val_fn: 7156.0000 - val_binary_accuracy: 0.7941 - val_precision: 0.7809 - val_recall: 0.7499 - val_auc: 0.8860\n",
      "Epoch 9/10\n",
      "3000/3000 - 15s - loss: 0.4570 - tp: 47403.0000 - fp: 15978.0000 - tn: 105191.0000 - fn: 23428.0000 - binary_accuracy: 0.7948 - precision: 0.7479 - recall: 0.6692 - auc: 0.8376 - val_loss: 0.4236 - val_tp: 21455.0000 - val_fp: 6023.0000 - val_tn: 29370.0000 - val_fn: 7152.0000 - val_binary_accuracy: 0.7941 - val_precision: 0.7808 - val_recall: 0.7500 - val_auc: 0.8865\n",
      "Epoch 10/10\n",
      "3000/3000 - 15s - loss: 0.4571 - tp: 47404.0000 - fp: 15975.0000 - tn: 105184.0000 - fn: 23437.0000 - binary_accuracy: 0.7947 - precision: 0.7479 - recall: 0.6692 - auc: 0.8375 - val_loss: 0.4237 - val_tp: 21457.0000 - val_fp: 6022.0000 - val_tn: 29370.0000 - val_fn: 7151.0000 - val_binary_accuracy: 0.7942 - val_precision: 0.7809 - val_recall: 0.7500 - val_auc: 0.8864\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2021-04-08 23:17:00.821652: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/michal/venv/ML-3.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:absl:Training complete. Model written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/Trainer/model/8/serving_model_dir. ModelRun written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/Trainer/model_run/8\n",
      "INFO:absl:Running publisher for Trainer\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Trainer is finished.\n",
      "INFO:absl:Component Evaluator is running.\n",
      "INFO:absl:Running driver for Evaluator\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "2021-04-08 23:17:02.393397: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "2021-04-08 23:17:02.395705: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "INFO:absl:Running executor for Evaluator\n",
      "WARNING:absl:\"maybe_add_baseline\" and \"maybe_remove_baseline\" are deprecated,\n",
      "        please use \"has_baseline\" instead.\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"Survived\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.5\n",
      "        }\n",
      "        upper_bound {\n",
      "          value: 0.99\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using /home/michal/artifact-store/tfx-titanic-training/20210408_231408/Trainer/model/8/serving_model_dir as  model.\n",
      "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
      "INFO:absl:Evaluating model.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7fa7a0239e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7fa7a0239e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Evaluation complete. Results written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/Evaluator/evaluation/9.\n",
      "INFO:absl:Checking validation results.\n",
      "INFO:absl:Blessing result True written to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/Evaluator/blessing/9.\n",
      "INFO:absl:Running publisher for Evaluator\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Evaluator is finished.\n",
      "INFO:absl:Component InfraValidator is running.\n",
      "INFO:absl:Running driver for InfraValidator\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "2021-04-08 23:17:08.201744: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "INFO:absl:Running executor for InfraValidator\n",
      "INFO:absl:InfraValidator will be run in LOAD_AND_QUERY mode.\n",
      "INFO:absl:tag_set is not given. Using {'serve'} instead.\n",
      "INFO:absl:signature_names are not given. Using ['serving_default'] instead.\n",
      "INFO:absl:Creating temp directory at /home/michal/artifact-store/tfx-titanic-training/20210408_231408/.temp/10/\n",
      "INFO:absl:Starting infra validation (attempt 1/3).\n",
      "INFO:absl:Starting LocalDockerRunner(image: tensorflow/serving:latest).\n",
      "INFO:absl:Running container with parameter {'auto_remove': True, 'detach': True, 'publish_all_ports': True, 'image': 'tensorflow/serving:latest', 'environment': {'MODEL_NAME': 'infra-validation-model', 'MODEL_BASE_PATH': '/model'}, 'mounts': [{'Target': '/model/infra-validation-model/1', 'Source': '/home/michal/artifact-store/tfx-titanic-training/20210408_231408/.temp/10/infra-validation-model/1617916628', 'Type': 'bind', 'ReadOnly': True}]}\n",
      "INFO:absl:Error while obtaining model status:\n",
      "<_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.UNAVAILABLE\n",
      "\tdetails = \"failed to connect to all addresses\"\n",
      "\tdebug_error_string = \"{\"created\":\"@1617916629.549756111\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":5396,\"referenced_errors\":[{\"created\":\"@1617916629.549754010\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":397,\"grpc_status\":14}]}\"\n",
      ">\n",
      "INFO:absl:Waiting for model to be loaded...\n",
      "INFO:absl:Model is successfully loaded.\n",
      "INFO:absl:Stopping LocalDockerRunner(image: tensorflow/serving:latest).\n",
      "INFO:absl:Stopping container.\n",
      "INFO:absl:Model passed infra validation.\n",
      "INFO:absl:Running publisher for InfraValidator\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component InfraValidator is finished.\n",
      "INFO:absl:Component Pusher is running.\n",
      "INFO:absl:Running driver for Pusher\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "2021-04-08 23:17:21.291043: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:588] No property is defined for the Type\n",
      "INFO:absl:Running executor for Pusher\n",
      "INFO:absl:Model version: 1617916641\n",
      "INFO:absl:Model written to serving path /home/michal/serving_model/1617916641.\n",
      "INFO:absl:Model pushed to /home/michal/artifact-store/tfx-titanic-training/20210408_231408/Pusher/pushed_model/11.\n",
      "INFO:absl:Running publisher for Pusher\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Pusher is finished.\n"
     ]
    }
   ],
   "source": [
    "#import local_runner\n",
    "\n",
    "\n",
    "!python local_runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run docker container for serving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm -p 8500:8500 -p 8501:8501 -p 8503:8503 -v=1 \\\n",
    " --mount type=bind,source=/home/michal/artifact-store/tfx-titanic-training/20210329_231949/Pusher/pushed_model/,target=/models/tfx_titanic_classifier \\\n",
    " -e MODEL_NAME=tfx_titanic_classifier -t tensorflow/serving:latest\n",
    "                \n",
    "#!docker run --rm -p 8500:8500 -p 8501:8501 -p 8503:8503 -v=1 \\\n",
    "# --mount type=bind,source=/home/michal/artifact-store/tfx-titanic-training/20210329_231949/Pusher/pushed_model/,target=/models/tfx_titanic_classifier \\\n",
    "# -e MODEL_NAME=tfx_titanic_classifier -t tensorflow/serving:latest\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for serializing data to tf.train.Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import features\n",
    "import importlib\n",
    "importlib.reload(features)\n",
    "#import tft \n",
    "import tensorflow_transform as tft\n",
    "\n",
    "feature_tf_example_mapping = {\n",
    "        'Embarked': _bytes_feature,\n",
    "        'Ticket': _bytes_feature,\n",
    "        'Sex': _bytes_feature,\n",
    "        'Name': _bytes_feature,\n",
    "        'Cabin': _bytes_feature,\n",
    "        'Age': _float_feature,\n",
    "        'Fare': _float_feature,\n",
    "        'Parch': _int64_feature,\n",
    "        'PassengerId': _int64_feature,\n",
    "        'Pclass': _int64_feature,\n",
    "        'SibSp': _int64_feature\n",
    "    }\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  if isinstance(value, str):\n",
    "    value = str.encode(value) # str wont work, we need bytes\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(data):\n",
    "  \"\"\"\n",
    "  Creates a tf.train.Example message ready to be written to a file.\n",
    "  data : dict\n",
    "            dictionary with data in key: value format\n",
    "  \"\"\"\n",
    "  if isinstance(data, pd.core.frame.DataFrame):\n",
    "        data = data.to_dict(orient='records')\n",
    "  \n",
    "  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "  # data type.\n",
    "  feature = { key: feature_tf_example_mapping[key](data[key]) for key in data.keys()}\n",
    "                                              \n",
    "  # Create a Features message using tf.train.Example.\n",
    "\n",
    "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example_proto.SerializeToString()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "\n",
    "from tensorflow.core.framework import types_pb2\n",
    "from tensorflow.core.framework import tensor_pb2\n",
    "from tensorflow.core.framework import tensor_shape_pb2\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc \n",
    "\n",
    "\n",
    "def predict_titanic(request_data):\n",
    "    \n",
    "    serialized_examples_array = [serialize_example(row) for row in request_data] # array od serialized examples\n",
    "    server = 'localhost:8500'\n",
    "    host, port = server.split(':')\n",
    "\n",
    "    channel = grpc.insecure_channel(server)\n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    \n",
    "    dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=len(request_data))]\n",
    "    tensor_shape_proto = tensor_shape_pb2.TensorShapeProto(dim=dims)\n",
    "    tensor_proto = tensor_pb2.TensorProto(\n",
    "                dtype=types_pb2.DT_STRING,\n",
    "                tensor_shape=tensor_shape_proto,\n",
    "                string_val=serialized_examples_array)\n",
    "\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = \"tfx_titanic_classifier\"\n",
    "    request.model_spec.signature_name = 'serving_default'\n",
    "    request.inputs['examples'].CopyFrom(tensor_proto)\n",
    "    result_future = stub.Predict(request, 30.)\n",
    "    \n",
    "    return result_future\n",
    "\n",
    "def parse_prediction_result(prediction_result):\n",
    "    outputs_tensor_proto = prediction_result.outputs[\"output_0\"]\n",
    "    shape = tf.TensorShape(outputs_tensor_proto.tensor_shape)\n",
    "    outputs = np.array(outputs_tensor_proto.float_val).reshape(shape)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_types  = {\n",
    "    'PassengerId': np.int32,\n",
    "    'Pclass': np.int32,\n",
    "    'Name': np.object,\n",
    "    'Sex': np.object,\n",
    "    'Age': np.float32,\n",
    "    'SibSp': np.int32,\n",
    "    'Parch': np.int32,\n",
    "    'Ticket': np.object,\n",
    "    'Fare': np.float32,\n",
    "    'Cabin': np.object,\n",
    "    'Embarked': np.object,\n",
    "    'Survived': np.int32,\n",
    "}\n",
    "converters = {'Cabin': str, 'Name': str, 'Ticket': str, 'Sex': str, 'Embarked': str}\n",
    "\n",
    "titanic_test_df = pd.read_csv(local_test_filepath, converters=converters)\n",
    "titanic_train_df = pd.read_csv(local_train_filepath, converters=converters)\n",
    "                     \n",
    "#titanic_test_df.head(10)\n",
    "\n",
    "titanic_train_df_survived = titanic_train_df[titanic_train_df['Survived'] == 1]\n",
    "titanic_train_df_dead = titanic_train_df[titanic_train_df['Survived'] == 0]\n",
    "\n",
    "train_survived_examples_df =  titanic_train_df_survived.head(10)\n",
    "train_dead_examples_df =  titanic_train_df_dead.head(10)\n",
    "\n",
    "train_survived_examples_data = train_survived_examples_df.to_dict(orient='records')\n",
    "train_dead_examples_data = train_dead_examples_df.to_dict(orient='records')\n",
    "\n",
    "#remove Survived label\n",
    "for example in train_survived_examples_data:\n",
    "    example.pop('Survived', None)\n",
    "for example in train_dead_examples_data:\n",
    "    example.pop('Survived', None)\n",
    "\n",
    "prediction_result_for_survived = predict_titanic(train_survived_examples_data)\n",
    "prediction_result_for_dead = predict_titanic(train_dead_examples_data)\n",
    "\n",
    "parsed_prediction_results_survived = parse_prediction_result(prediction_result_for_survived)\n",
    "parsed_prediction_results_dead = parse_prediction_result(prediction_result_for_dead)\n",
    "\n",
    "train_survived_examples_df['Survived_prediction'] = parsed_prediction_results_survived\n",
    "train_dead_examples_df['Survived_prediction'] = parsed_prediction_results_dead\n",
    "\n",
    "#pprint(train_survived_examples_df)\n",
    "display(train_survived_examples_df)\n",
    "display(train_dead_examples_df)\n",
    "print(parsed_prediction_results_survived)\n",
    "print(parsed_prediction_results_dead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first10 = titanic.iloc[:10].to_dict(orient='records')\n",
    "prediction_result = predict_titanic(first10)\n",
    "type(prediction_result)\n",
    "#prediction_result.outputs.values\n",
    "outputs_tensor_proto = prediction_result.outputs[\"output_0\"]\n",
    "print(outputs_tensor_proto)\n",
    "\n",
    "prediction_result\n",
    "\n",
    "\n",
    "shape = tf.TensorShape(outputs_tensor_proto.tensor_shape)\n",
    "#outputs = tf.constant(outputs_tensor_proto.float_val, shape=shape)\n",
    "outputs = np.array(outputs_tensor_proto.float_val).reshape(shape)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
